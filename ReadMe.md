# RAG-Power Survey Analysis

## Overview

RAG-Power Survey Analysis is an application that leverages Retrieval-Augmented Generation (RAG) to analyze and compare market research reports from multiple PDFs. The backend is built using FastAPI, while the frontend is developed with Streamlit. The system extracts text from PDFs (with OCR fallback), chunks the text, creates vector embeddings, and uses AI-powered models (via HuggingFace endpoints) to answer questions and compare reports—all while displaying source references for transparency.

## Features

- **PDF Upload & Processing:**  
  Upload multiple PDFs. The system extracts text from each page (using PyPDF2 and OCR as needed), and splits the text into manageable chunks.

- **AI-Powered Q&A:**  
  Ask questions and receive answers generated by AI models, with source citations that reference the original PDFs and pages.

- **Report Comparison:**  
  Compare two market research reports side-by-side, with individual summaries and a combined analysis highlighting similarities, differences, and key insights.

- **Enhanced Frontend UX:**  
  A clean, modern UI built with Streamlit, featuring custom CSS using HEX colors `#e14ed2` and `#3edbda`, loading spinners, and intuitive error messages.

- **Secure API Key Management:**  
  Sensitive API keys are loaded from environment variables (using python-dotenv), keeping them out of the source code.

## Technologies Used

- **Backend:** Python, FastAPI, Uvicorn
- **Frontend:** Streamlit
- **AI/NLP:** Langchain, HuggingFace endpoints, FAISS, PyPDF2, pdf2image, pytesseract
- **Deployment:** Docker, Docker Compose (optional), Heroku/Render (optional)
- **Environment Management:** python-dotenv

## Project Structure
├── api.py                  # FastAPI backend application
├── client.py               # Streamlit frontend application
├── Vectorstore.py          # PDF text extraction, chunking, and FAISS vectorstore creation
├── Llama3.py               # HuggingFace endpoint for Llama 3.1 (API key managed via environment variable)
├── Mixtral.py              # HuggingFace endpoint for Mixtral model (API key managed via environment variable)
├── Phi.py                  # HuggingFace endpoint for Phi model (API key managed via environment variable)
├── Dockerfile.backend      # Dockerfile for backend deployment
├── Dockerfile.client       # Dockerfile for frontend deployment
├── docker-compose.yml      # Docker Compose configuration for multi-container deployment
├── requirements.txt        # Python dependencies for the backend
├── requirements_client.txt # Python dependencies for the frontend
├── .env                    # Environment variables file (not committed)
└── README.md               # This file